{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_recommenders\n",
      "  Downloading tensorflow_recommenders-0.6.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 607 kB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow_recommenders) (2.6.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow_recommenders) (0.14.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from absl-py>=0.1.6->tensorflow_recommenders) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.40.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (0.37.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.1.2)\n",
      "Requirement already satisfied: clang~=5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (5.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (3.18.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.19.5)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (3.3.0)\n",
      "Requirement already satisfied: keras~=2.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (56.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.6.0->tensorflow_recommenders) (3.1.1)\n",
      "Installing collected packages: typing-extensions, tensorflow-recommenders\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylint 2.11.1 requires typing-extensions>=3.10.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "astroid 2.8.0 requires typing-extensions>=3.10; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\n",
      "Successfully installed tensorflow-recommenders-0.6.0 typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the MovieLens dataset to train a simple model for movie recommendations. This dataset contains information on what movies a user watched, and what ratings users gave to the movies they watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 20:19:23.058371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 20:19:25.518707: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-29 20:19:25.560487: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 20:19:27.873878: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {movie_title: (), user_id: ()}, types: {movie_title: tf.string, user_id: tf.string}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
       "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
       "       b'2 Days in the Valley (1996)',\n",
       "       b'20,000 Leagues Under the Sea (1954)',\n",
       "       b'2001: A Space Odyssey (1968)',\n",
       "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
       "       b'39 Steps, The (1935)'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_movie_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_movie_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoBaseClassMovielensModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    # Set up a gradient tape to record gradients.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Loss computation.\n",
    "      user_embeddings = self.user_model(features[\"user_id\"])\n",
    "      positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "      loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "      # Handle regularization losses as well.\n",
    "      regularization_loss = sum(self.losses)\n",
    "\n",
    "      total_loss = loss + regularization_loss\n",
    "\n",
    "    gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "    metrics[\"loss\"] = loss\n",
    "    metrics[\"regularization_loss\"] = regularization_loss\n",
    "    metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "    return metrics\n",
    "\n",
    "  def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    # Loss computation.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "    loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "    # Handle regularization losses as well.\n",
    "    regularization_loss = sum(self.losses)\n",
    "\n",
    "    total_loss = loss + regularization_loss\n",
    "\n",
    "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "    metrics[\"loss\"] = loss\n",
    "    metrics[\"regularization_loss\"] = regularization_loss\n",
    "    metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 10s 725ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0097 - factorized_top_k/top_10_categorical_accuracy: 0.0206 - factorized_top_k/top_50_categorical_accuracy: 0.1005 - factorized_top_k/top_100_categorical_accuracy: 0.1776 - loss: 69885.1129 - regularization_loss: 0.0000e+00 - total_loss: 69885.1129\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 6s 636ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0029 - factorized_top_k/top_5_categorical_accuracy: 0.0189 - factorized_top_k/top_10_categorical_accuracy: 0.0380 - factorized_top_k/top_50_categorical_accuracy: 0.1696 - factorized_top_k/top_100_categorical_accuracy: 0.2931 - loss: 67523.3707 - regularization_loss: 0.0000e+00 - total_loss: 67523.3707\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 7s 726ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0037 - factorized_top_k/top_5_categorical_accuracy: 0.0224 - factorized_top_k/top_10_categorical_accuracy: 0.0459 - factorized_top_k/top_50_categorical_accuracy: 0.1883 - factorized_top_k/top_100_categorical_accuracy: 0.3162 - loss: 66302.9609 - regularization_loss: 0.0000e+00 - total_loss: 66302.9609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcad3ff0bb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 332ms/step - factorized_top_k/top_1_categorical_accuracy: 8.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0094 - factorized_top_k/top_10_categorical_accuracy: 0.0224 - factorized_top_k/top_50_categorical_accuracy: 0.1245 - factorized_top_k/top_100_categorical_accuracy: 0.2327 - loss: 31079.0635 - regularization_loss: 0.0000e+00 - total_loss: 31079.0635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0008500000112690032,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.009399999864399433,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.022350000217556953,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.124549999833107,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.23270000517368317,\n",
       " 'loss': 28244.771484375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 28244.771484375}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:00:50.369539: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ch/bbgk8vxn5nj22fhbt1wttjwm0000gn/T/tmphpyl4f04/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ch/bbgk8vxn5nj22fhbt1wttjwm0000gn/T/tmphpyl4f04/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "\n",
    "  # Save the index.\n",
    "  tf.saved_model.save(index, path)\n",
    "\n",
    "  # Load it back; can also be done in TensorFlow Serving.\n",
    "  loaded = tf.saved_model.load(path)\n",
    "\n",
    "  # Pass a user id in, get top predicted movie titles back.\n",
    "  scores, titles = loaded([\"42\"])\n",
    "\n",
    "  print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\n",
    "scann_index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations.\n",
    "_, titles = scann_index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "\n",
    "  # Save the index.\n",
    "  tf.saved_model.save(\n",
    "      index,\n",
    "      path,\n",
    "      options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    "  )\n",
    "\n",
    "  # Load it back; can also be done in TensorFlow Serving.\n",
    "  loaded = tf.saved_model.load(path)\n",
    "\n",
    "  # Pass a user id in, get top predicted movie titles back.\n",
    "  scores, titles = loaded([\"42\"])\n",
    "\n",
    "  print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
